# åç«¯å¼€å‘æŒ‡å—

## ğŸ—ï¸ åç«¯æ¶æ„æ¦‚è¿°

åç«¯é‡‡ç”¨ **FastAPI + LangChain** æ¶æ„ï¼Œè´Ÿè´£éŸ³é¢‘åˆ†æã€è¡¨æƒ…ç”Ÿæˆå’Œ API æœåŠ¡ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ï¼š

- **éŸ³é¢‘åˆ†ææ¨¡å—**: ä½¿ç”¨ librosa è¿›è¡ŒéŸ³é¢‘ç‰¹å¾æå–
- **LangChain ä»£ç†**: åŸºäº AI çš„è¡¨æƒ…æ˜ å°„ç”Ÿæˆ
- **FastAPI æœåŠ¡**: RESTful API æ¥å£æä¾›
- **æ•°æ®æ¨¡å‹**: Pydantic æ•°æ®éªŒè¯å’Œåºåˆ—åŒ–

## ğŸ“ åç«¯ç›®å½•ç»“æ„

```
backend/
â”œâ”€â”€ core/                           # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ audio_analyzer.py          # éŸ³é¢‘åˆ†æå™¨
â”‚   â”œâ”€â”€ expression_generator.py    # è¡¨æƒ…ç”Ÿæˆå™¨
â”‚   â”œâ”€â”€ langchain_agent.py         # LangChainä»£ç†
â”‚   â””â”€â”€ live2d_controller.py       # Live2Dæ§åˆ¶å™¨
â”œâ”€â”€ api/                           # APIè·¯ç”±å’Œæ¥å£
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                   # FastAPIä¸»åº”ç”¨
â”‚   â”œâ”€â”€ routes/                   # è·¯ç”±æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ upload.py            # æ–‡ä»¶ä¸Šä¼ è·¯ç”±
â”‚   â”‚   â”œâ”€â”€ analyze.py           # åˆ†æå¤„ç†è·¯ç”±
â”‚   â”‚   â””â”€â”€ expression.py        # è¡¨æƒ…ç›¸å…³è·¯ç”±
â”‚   â””â”€â”€ dependencies.py          # ä¾èµ–æ³¨å…¥
â”œâ”€â”€ models/                      # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ audio.py                # éŸ³é¢‘ç›¸å…³æ¨¡å‹
â”‚   â”œâ”€â”€ expression.py           # è¡¨æƒ…ç›¸å…³æ¨¡å‹
â”‚   â””â”€â”€ response.py             # å“åº”æ¨¡å‹
â”œâ”€â”€ utils/                      # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ file_utils.py          # æ–‡ä»¶å¤„ç†å·¥å…·
â”‚   â”œâ”€â”€ audio_utils.py         # éŸ³é¢‘å¤„ç†å·¥å…·
â”‚   â””â”€â”€ config.py              # é…ç½®ç®¡ç†
â””â”€â”€ tests/                     # åç«¯æµ‹è¯•
    â”œâ”€â”€ test_audio_analyzer.py
    â”œâ”€â”€ test_expression_generator.py
    â””â”€â”€ test_api.py
```

## ğŸ”§ æ ¸å¿ƒæ¨¡å—å¼€å‘

### 1. éŸ³é¢‘åˆ†æå™¨ (AudioAnalyzer)

#### `backend/core/audio_analyzer.py`

```python
import librosa
import numpy as np
from typing import Dict, List, Tuple
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class AudioFeatures:
    """éŸ³é¢‘ç‰¹å¾æ•°æ®ç±»"""
    duration: float
    tempo: float
    beats: List[float]
    pitch: List[float]
    energy: List[float]
    spectral_centroid: List[float]
    mfcc: np.ndarray
    emotion_scores: Dict[str, float]
    timestamps: List[float]

class AudioAnalyzer:
    """éŸ³é¢‘åˆ†æå™¨"""

    def __init__(self, sample_rate: int = 44100, hop_length: int = 512):
        self.sample_rate = sample_rate
        self.hop_length = hop_length
        self.frame_length = 2048

    def analyze(self, audio_path: str) -> AudioFeatures:
        """
        åˆ†æéŸ³é¢‘æ–‡ä»¶ï¼Œæå–æ‰€æœ‰ç‰¹å¾

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            AudioFeatures: æå–çš„éŸ³é¢‘ç‰¹å¾
        """
        logger.info(f"å¼€å§‹åˆ†æéŸ³é¢‘æ–‡ä»¶: {audio_path}")

        try:
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(audio_path, sr=self.sample_rate)
            duration = librosa.get_duration(y=y, sr=sr)

            # æå–å„ç§ç‰¹å¾
            tempo, beats = self._extract_tempo_and_beats(y, sr)
            pitch = self._extract_pitch(y, sr)
            energy = self._extract_energy(y, sr)
            spectral_centroid = self._extract_spectral_centroid(y, sr)
            mfcc = self._extract_mfcc(y, sr)
            emotion_scores = self._analyze_emotion(y, sr)

            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamps = librosa.frames_to_time(
                np.arange(len(energy)),
                sr=sr,
                hop_length=self.hop_length
            ).tolist()

            features = AudioFeatures(
                duration=duration,
                tempo=tempo,
                beats=beats.tolist(),
                pitch=pitch.tolist(),
                energy=energy.tolist(),
                spectral_centroid=spectral_centroid.tolist(),
                mfcc=mfcc,
                emotion_scores=emotion_scores,
                timestamps=timestamps
            )

            logger.info(f"éŸ³é¢‘åˆ†æå®Œæˆï¼Œæ—¶é•¿: {duration:.2f}ç§’, BPM: {tempo:.1f}")
            return features

        except Exception as e:
            logger.error(f"éŸ³é¢‘åˆ†æå¤±è´¥: {str(e)}")
            raise

    def _extract_tempo_and_beats(self, y: np.ndarray, sr: int) -> Tuple[float, np.ndarray]:
        """æå–èŠ‚æ‹å’ŒBPM"""
        tempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=self.hop_length)
        beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=self.hop_length)
        return float(tempo), beat_times

    def _extract_pitch(self, y: np.ndarray, sr: int) -> np.ndarray:
        """æå–éŸ³é«˜"""
        pitches, magnitudes = librosa.piptrack(
            y=y, sr=sr, hop_length=self.hop_length
        )

        # æå–ä¸»è¦éŸ³é«˜
        pitch_values = []
        for t in range(pitches.shape[1]):
            index = magnitudes[:, t].argmax()
            pitch = pitches[index, t] if magnitudes[index, t] > 0 else 0
            pitch_values.append(pitch)

        return np.array(pitch_values)

    def _extract_energy(self, y: np.ndarray, sr: int) -> np.ndarray:
        """æå–èƒ½é‡"""
        # RMSèƒ½é‡
        rms = librosa.feature.rms(
            y=y, hop_length=self.hop_length, frame_length=self.frame_length
        )[0]

        # å½’ä¸€åŒ–åˆ°0-1
        rms = rms / np.max(rms) if np.max(rms) > 0 else rms
        return rms

    def _extract_spectral_centroid(self, y: np.ndarray, sr: int) -> np.ndarray:
        """æå–é¢‘è°±è´¨å¿ƒ"""
        spectral_centroid = librosa.feature.spectral_centroid(
            y=y, sr=sr, hop_length=self.hop_length
        )[0]

        # å½’ä¸€åŒ–
        spectral_centroid = spectral_centroid / np.max(spectral_centroid)
        return spectral_centroid

    def _extract_mfcc(self, y: np.ndarray, sr: int, n_mfcc: int = 13) -> np.ndarray:
        """æå–MFCCç‰¹å¾"""
        mfcc = librosa.feature.mfcc(
            y=y, sr=sr, n_mfcc=n_mfcc, hop_length=self.hop_length
        )
        return mfcc

    def _analyze_emotion(self, y: np.ndarray, sr: int) -> Dict[str, float]:
        """
        ç®€å•çš„æƒ…æ„Ÿåˆ†æ
        åŸºäºéŸ³é¢‘ç‰¹å¾æ¨æ–­æƒ…æ„ŸçŠ¶æ€
        """
        # æå–ç‰¹å¾ç”¨äºæƒ…æ„Ÿåˆ†æ
        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
        energy = np.mean(librosa.feature.rms(y=y, hop_length=self.hop_length))
        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))
        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y))

        # ç®€å•çš„è§„åˆ™åŸºç¡€æƒ…æ„Ÿåˆ†æ
        emotion_scores = {
            'happy': 0.0,
            'sad': 0.0,
            'energetic': 0.0,
            'calm': 0.0,
            'angry': 0.0
        }

        # åŸºäºèŠ‚æ‹åˆ¤æ–­
        if tempo > 120:
            emotion_scores['happy'] += 0.3
            emotion_scores['energetic'] += 0.4
        elif tempo < 80:
            emotion_scores['sad'] += 0.3
            emotion_scores['calm'] += 0.4

        # åŸºäºèƒ½é‡åˆ¤æ–­
        if energy > 0.1:
            emotion_scores['energetic'] += 0.3
            emotion_scores['angry'] += 0.2
        else:
            emotion_scores['calm'] += 0.3
            emotion_scores['sad'] += 0.2

        # åŸºäºé¢‘è°±è´¨å¿ƒåˆ¤æ–­
        if spectral_centroid > 3000:
            emotion_scores['happy'] += 0.2
            emotion_scores['energetic'] += 0.2
        else:
            emotion_scores['sad'] += 0.2
            emotion_scores['calm'] += 0.2

        # å½’ä¸€åŒ–æƒ…æ„Ÿåˆ†æ•°
        total_score = sum(emotion_scores.values())
        if total_score > 0:
            emotion_scores = {k: v/total_score for k, v in emotion_scores.items()}

        return emotion_scores

    def extract_features_at_time(self, audio_path: str, time_point: float) -> Dict:
        """
        æå–æŒ‡å®šæ—¶é—´ç‚¹çš„éŸ³é¢‘ç‰¹å¾

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            time_point: æ—¶é—´ç‚¹ï¼ˆç§’ï¼‰

        Returns:
            Dict: è¯¥æ—¶é—´ç‚¹çš„ç‰¹å¾
        """
        y, sr = librosa.load(audio_path, sr=self.sample_rate)

        # è®¡ç®—å¯¹åº”çš„å¸§ç´¢å¼•
        frame = int(time_point * sr // self.hop_length)

        # æå–è¯¥å¸§çš„ç‰¹å¾
        rms = librosa.feature.rms(y=y, hop_length=self.hop_length)[0]
        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]

        if frame < len(rms):
            return {
                'time': time_point,
                'energy': float(rms[frame]),
                'spectral_centroid': float(spectral_centroid[frame]),
                'pitch': self._get_pitch_at_frame(y, sr, frame)
            }
        else:
            return None

    def _get_pitch_at_frame(self, y: np.ndarray, sr: int, frame: int) -> float:
        """è·å–æŒ‡å®šå¸§çš„éŸ³é«˜"""
        # ç®€åŒ–çš„éŸ³é«˜æå–
        start_sample = frame * self.hop_length
        end_sample = start_sample + self.frame_length

        if end_sample < len(y):
            segment = y[start_sample:end_sample]
            pitches, magnitudes = librosa.piptrack(
                y=segment, sr=sr, hop_length=self.hop_length
            )

            if pitches.size > 0 and magnitudes.size > 0:
                index = magnitudes[:, 0].argmax()
                return float(pitches[index, 0]) if magnitudes[index, 0] > 0 else 0.0

        return 0.0
```

### 2. LangChain ä»£ç† (ExpressionAgent)

#### `backend/core/langchain_agent.py`

```python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.schema import HumanMessage, SystemMessage
from typing import Dict, List, Any
import json
import logging
import os
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

class ExpressionAgent:
    """åŸºäºLangChainçš„è¡¨æƒ…ç”Ÿæˆä»£ç†"""

    def __init__(
        self,
        model_name: str = "gpt-3.5-turbo",
        temperature: float = 0.7,
        max_tokens: int = 1000
    ):
        """
        åˆå§‹åŒ–è¡¨æƒ…ä»£ç†

        Args:
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: åˆ›é€ æ€§å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens

        # åˆå§‹åŒ–LLM
        self.llm = ChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        # æ„å»ºè¡¨æƒ…ç”Ÿæˆé“¾
        self.expression_chain = self._build_expression_chain()

        # Live2Då‚æ•°æ˜ å°„
        self.live2d_params = {
            'eye_open': 'ParamEyeLOpen',
            'eye_open_r': 'ParamEyeROpen',
            'eyebrow_height': 'ParamEyeBrowLY',
            'eyebrow_height_r': 'ParamEyeBrowRY',
            'mouth_open': 'ParamMouthOpenY',
            'mouth_form': 'ParamMouthForm',
            'cheek': 'ParamCheek',
            'body_angle_x': 'ParamBodyAngleX',
            'body_angle_y': 'ParamBodyAngleY',
            'breath': 'ParamBreath'
        }

    def _build_expression_chain(self) -> LLMChain:
        """æ„å»ºè¡¨æƒ…ç”Ÿæˆé“¾"""

        system_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è™šæ‹Ÿäººè¡¨æƒ…è®¾è®¡å¸ˆï¼Œæ“…é•¿æ ¹æ®éŸ³ä¹ç‰¹å¾ä¸ºLive2Dè§’è‰²è®¾è®¡è¡¨æƒ…åŠ¨ç”»ã€‚

Live2Då‚æ•°è¯´æ˜ï¼š
- eye_open (çœ¼éƒ¨å¼€åˆ): 0.0=é—­çœ¼, 1.0=å®Œå…¨çå¼€
- eyebrow_height (çœ‰æ¯›é«˜åº¦): 0.0=ä¸‹å‚, 0.5=æ­£å¸¸, 1.0=ä¸Šæ‰¬
- mouth_open (å˜´éƒ¨å¼€åˆ): 0.0=é—­å˜´, 1.0=å¼ å¤§å˜´
- mouth_form (å˜´å‹): 0.0=é»˜è®¤, 0.5=å¾®ç¬‘, 1.0=å¤§ç¬‘
- cheek (è„¸é¢Šçº¢æ™•): 0.0=æ— çº¢æ™•, 1.0=æ»¡çº¢æ™•
- body_angle_x (èº«ä½“Xè½´è§’åº¦): -1.0åˆ°1.0ï¼Œå·¦å³æ‘†åŠ¨
- breath (å‘¼å¸): 0.0åˆ°1.0ï¼Œå‘¼å¸å¹…åº¦

è¡¨æƒ…è®¾è®¡åŸåˆ™ï¼š
1. æ ¹æ®éŸ³ä¹èŠ‚æ‹è°ƒæ•´çœ¨çœ¼å’Œç‚¹å¤´é¢‘ç‡
2. æ ¹æ®æƒ…æ„Ÿå¼ºåº¦è°ƒæ•´è¡¨æƒ…å¤¸å¼ ç¨‹åº¦
3. ä¿æŒè¡¨æƒ…è‡ªç„¶è¿‡æ¸¡ï¼Œé¿å…çªå…€å˜åŒ–
4. è€ƒè™‘éŸ³ä¹ç±»å‹çš„æ–‡åŒ–ç‰¹å¾
"""

        human_template = """
åŸºäºä»¥ä¸‹éŸ³ä¹ç‰¹å¾ï¼Œä¸ºè™šæ‹Ÿè§’è‰²è®¾è®¡è¡¨æƒ…å‚æ•°ï¼š

æ—¶é—´ç‚¹: {timestamp}ç§’
èŠ‚æ‹: {tempo} BPM
èƒ½é‡çº§åˆ«: {energy} (0-1)
é¢‘è°±è´¨å¿ƒ: {spectral_centroid} (0-1)
éŸ³é«˜: {pitch} Hz
æƒ…æ„Ÿåˆ†æ: {emotion_scores}

è¯·ç”ŸæˆLive2Dè¡¨æƒ…å‚æ•°ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "eye_open": 0.8,
    "eyebrow_height": 0.5,
    "mouth_open": 0.3,
    "mouth_form": 0.4,
    "cheek": 0.1,
    "body_angle_x": 0.0,
    "breath": 0.6,
    "transition_duration": 0.5,
    "reasoning": "åŸºäºå½“å‰éŸ³ä¹ç‰¹å¾çš„è®¾è®¡æ€è·¯"
}}

åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_template),
            ("human", human_template)
        ])

        return LLMChain(llm=self.llm, prompt=prompt)

    def generate_expression(self, audio_features: Dict) -> Dict:
        """
        åŸºäºéŸ³é¢‘ç‰¹å¾ç”Ÿæˆè¡¨æƒ…å‚æ•°

        Args:
            audio_features: éŸ³é¢‘ç‰¹å¾å­—å…¸

        Returns:
            Dict: Live2Dè¡¨æƒ…å‚æ•°
        """
        try:
            # è°ƒç”¨LLMç”Ÿæˆè¡¨æƒ…
            response = self.expression_chain.run(**audio_features)

            # è§£æJSONå“åº”
            expression_data = json.loads(response)

            # éªŒè¯å’Œä¿®æ­£å‚æ•°èŒƒå›´
            expression_data = self._validate_parameters(expression_data)

            # è½¬æ¢ä¸ºLive2Då‚æ•°å
            live2d_params = self._convert_to_live2d_params(expression_data)

            logger.info(f"ç”Ÿæˆè¡¨æƒ…å‚æ•°: {live2d_params}")
            return live2d_params

        except Exception as e:
            logger.error(f"è¡¨æƒ…ç”Ÿæˆå¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤è¡¨æƒ…
            return self._get_default_expression()

    def generate_expression_timeline(
        self,
        audio_features_timeline: List[Dict],
        smoothing: bool = True
    ) -> List[Dict]:
        """
        ç”Ÿæˆå®Œæ•´çš„è¡¨æƒ…æ—¶é—´è½´

        Args:
            audio_features_timeline: éŸ³é¢‘ç‰¹å¾æ—¶é—´è½´åˆ—è¡¨
            smoothing: æ˜¯å¦å¯ç”¨å¹³æ»‘å¤„ç†

        Returns:
            List[Dict]: è¡¨æƒ…å‚æ•°æ—¶é—´è½´
        """
        expression_timeline = []

        for i, features in enumerate(audio_features_timeline):
            try:
                # ç”Ÿæˆå½“å‰æ—¶é—´ç‚¹çš„è¡¨æƒ…
                expression = self.generate_expression(features)

                # æ·»åŠ æ—¶é—´æˆ³
                expression['timestamp'] = features.get('timestamp', 0)

                # å¦‚æœå¯ç”¨å¹³æ»‘å¤„ç†
                if smoothing and i > 0:
                    expression = self._smooth_expression(
                        expression_timeline[-1],
                        expression
                    )

                expression_timeline.append(expression)

            except Exception as e:
                logger.error(f"æ—¶é—´ç‚¹ {features.get('timestamp', 0)} è¡¨æƒ…ç”Ÿæˆå¤±è´¥: {str(e)}")
                continue

        logger.info(f"ç”Ÿæˆè¡¨æƒ…æ—¶é—´è½´ï¼Œå…± {len(expression_timeline)} ä¸ªå…³é”®å¸§")
        return expression_timeline

    def _validate_parameters(self, params: Dict) -> Dict:
        """éªŒè¯å’Œä¿®æ­£å‚æ•°èŒƒå›´"""
        valid_params = {}

        # å‚æ•°èŒƒå›´å®šä¹‰
        param_ranges = {
            'eye_open': (0.0, 1.0),
            'eyebrow_height': (0.0, 1.0),
            'mouth_open': (0.0, 1.0),
            'mouth_form': (0.0, 1.0),
            'cheek': (0.0, 1.0),
            'body_angle_x': (-1.0, 1.0),
            'breath': (0.0, 1.0),
            'transition_duration': (0.1, 3.0)
        }

        for param, (min_val, max_val) in param_ranges.items():
            if param in params:
                value = params[param]
                # ç¡®ä¿æ˜¯æ•°å­—
                if isinstance(value, (int, float)):
                    # é™åˆ¶èŒƒå›´
                    valid_params[param] = max(min_val, min(max_val, float(value)))
                else:
                    valid_params[param] = (min_val + max_val) / 2  # é»˜è®¤ä¸­é—´å€¼
            else:
                valid_params[param] = (min_val + max_val) / 2

        # ä¿ç•™æ¨ç†ä¿¡æ¯
        if 'reasoning' in params:
            valid_params['reasoning'] = params['reasoning']

        return valid_params

    def _convert_to_live2d_params(self, expression_params: Dict) -> Dict:
        """è½¬æ¢ä¸ºLive2Då‚æ•°æ ¼å¼"""
        live2d_data = {
            'parameters': {},
            'metadata': {}
        }

        # è½¬æ¢å‚æ•°
        for key, value in expression_params.items():
            if key in self.live2d_params:
                live2d_param_name = self.live2d_params[key]
                live2d_data['parameters'][live2d_param_name] = value
            elif key == 'transition_duration':
                live2d_data['metadata']['transition_duration'] = value
            elif key == 'reasoning':
                live2d_data['metadata']['reasoning'] = value

        return live2d_data

    def _smooth_expression(self, prev_expression: Dict, curr_expression: Dict) -> Dict:
        """è¡¨æƒ…å¹³æ»‘å¤„ç†"""
        smoothed = curr_expression.copy()

        # è·å–å‰ä¸€å¸§çš„å‚æ•°
        prev_params = prev_expression.get('parameters', {})
        curr_params = curr_expression.get('parameters', {})

        # å¹³æ»‘å› å­
        alpha = 0.3

        # å¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œå¹³æ»‘
        for param_name in curr_params:
            if param_name in prev_params:
                prev_val = prev_params[param_name]
                curr_val = curr_params[param_name]
                # çº¿æ€§æ’å€¼å¹³æ»‘
                smoothed_val = prev_val * (1 - alpha) + curr_val * alpha
                smoothed['parameters'][param_name] = smoothed_val

        return smoothed

    def _get_default_expression(self) -> Dict:
        """è·å–é»˜è®¤è¡¨æƒ…"""
        return {
            'parameters': {
                'ParamEyeLOpen': 1.0,
                'ParamEyeROpen': 1.0,
                'ParamEyeBrowLY': 0.5,
                'ParamEyeBrowRY': 0.5,
                'ParamMouthOpenY': 0.2,
                'ParamMouthForm': 0.3,
                'ParamCheek': 0.0,
                'ParamBodyAngleX': 0.0,
                'ParamBreath': 0.5
            },
            'metadata': {
                'transition_duration': 0.5,
                'reasoning': 'é»˜è®¤ä¸­æ€§è¡¨æƒ…'
            }
        }

    def customize_expression_rules(self, custom_rules: Dict) -> None:
        """
        è‡ªå®šä¹‰è¡¨æƒ…æ˜ å°„è§„åˆ™

        Args:
            custom_rules: è‡ªå®šä¹‰è§„åˆ™å­—å…¸
        """
        # è¿™é‡Œå¯ä»¥å®ç°ç”¨æˆ·è‡ªå®šä¹‰çš„è¡¨æƒ…æ˜ å°„è§„åˆ™
        # ä¾‹å¦‚ï¼šç‰¹å®šBPMèŒƒå›´å¯¹åº”ç‰¹å®šè¡¨æƒ…å¼ºåº¦
        pass
```

### 3. è¡¨æƒ…ç”Ÿæˆå™¨ (ExpressionGenerator)

#### `backend/core/expression_generator.py`

```python
import json
import numpy as np
from typing import Dict, List, Optional
from pathlib import Path
import logging
from .audio_analyzer import AudioAnalyzer, AudioFeatures
from .langchain_agent import ExpressionAgent

logger = logging.getLogger(__name__)

class ExpressionGenerator:
    """è¡¨æƒ…æ–‡ä»¶ç”Ÿæˆå™¨"""

    def __init__(
        self,
        analyzer: Optional[AudioAnalyzer] = None,
        agent: Optional[ExpressionAgent] = None
    ):
        """
        åˆå§‹åŒ–è¡¨æƒ…ç”Ÿæˆå™¨

        Args:
            analyzer: éŸ³é¢‘åˆ†æå™¨å®ä¾‹
            agent: è¡¨æƒ…ä»£ç†å®ä¾‹
        """
        self.analyzer = analyzer or AudioAnalyzer()
        self.agent = agent or ExpressionAgent()

    def generate_expression_file(
        self,
        audio_path: str,
        output_path: str,
        model_name: str = "default",
        time_resolution: float = 0.5,
        enable_smoothing: bool = True
    ) -> Dict:
        """
        ç”Ÿæˆå®Œæ•´çš„Live2Dè¡¨æƒ…æ–‡ä»¶

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºè¡¨æƒ…æ–‡ä»¶è·¯å¾„
            model_name: Live2Dæ¨¡å‹åç§°
            time_resolution: æ—¶é—´åˆ†è¾¨ç‡ï¼ˆç§’ï¼‰
            enable_smoothing: æ˜¯å¦å¯ç”¨å¹³æ»‘å¤„ç†

        Returns:
            Dict: ç”Ÿæˆç»“æœä¿¡æ¯
        """
        try:
            logger.info(f"å¼€å§‹ç”Ÿæˆè¡¨æƒ…æ–‡ä»¶: {audio_path} -> {output_path}")

            # 1. åˆ†æéŸ³é¢‘ç‰¹å¾
            logger.info("æ­¥éª¤1: åˆ†æéŸ³é¢‘ç‰¹å¾")
            audio_features = self.analyzer.analyze(audio_path)

            # 2. æ„å»ºæ—¶é—´è½´
            logger.info("æ­¥éª¤2: æ„å»ºæ—¶é—´è½´")
            timeline = self._build_timeline(audio_features, time_resolution)

            # 3. ç”Ÿæˆè¡¨æƒ…åºåˆ—
            logger.info("æ­¥éª¤3: ç”Ÿæˆè¡¨æƒ…åºåˆ—")
            expressions = self.agent.generate_expression_timeline(
                timeline,
                smoothing=enable_smoothing
            )

            # 4. æ„å»ºæœ€ç»ˆçš„è¡¨æƒ…æ–‡ä»¶
            logger.info("æ­¥éª¤4: æ„å»ºè¡¨æƒ…æ–‡ä»¶")
            expression_file = self._build_expression_file(
                audio_features,
                expressions,
                model_name
            )

            # 5. ä¿å­˜æ–‡ä»¶
            logger.info("æ­¥éª¤5: ä¿å­˜è¡¨æƒ…æ–‡ä»¶")
            self._save_expression_file(expression_file, output_path)

            result = {
                'success': True,
                'output_path': output_path,
                'duration': audio_features.duration,
                'expression_count': len(expressions),
                'model_name': model_name,
                'metadata': {
                    'tempo': audio_features.tempo,
                    'emotion_scores': audio_features.emotion_scores,
                    'time_resolution': time_resolution
                }
            }

            logger.info(f"è¡¨æƒ…æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {len(expressions)} ä¸ªå…³é”®å¸§")
            return result

        except Exception as e:
            logger.error(f"è¡¨æƒ…æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'output_path': output_path
            }

    def _build_timeline(self, audio_features: AudioFeatures, time_resolution: float) -> List[Dict]:
        """
        æ„å»ºéŸ³é¢‘ç‰¹å¾æ—¶é—´è½´

        Args:
            audio_features: éŸ³é¢‘ç‰¹å¾
            time_resolution: æ—¶é—´åˆ†è¾¨ç‡

        Returns:
            List[Dict]: æ—¶é—´è½´ç‰¹å¾åˆ—è¡¨
        """
        timeline = []
        duration = audio_features.duration

        # ç”Ÿæˆæ—¶é—´ç‚¹
        time_points = np.arange(0, duration, time_resolution)

        for time_point in time_points:
            # æ‰¾åˆ°æœ€æ¥è¿‘çš„ç‰¹å¾ç´¢å¼•
            frame_idx = int(time_point * len(audio_features.timestamps) / duration)
            frame_idx = min(frame_idx, len(audio_features.timestamps) - 1)

            # æ„å»ºè¯¥æ—¶é—´ç‚¹çš„ç‰¹å¾
            timeline_point = {
                'timestamp': float(time_point),
                'tempo': audio_features.tempo,
                'energy': float(audio_features.energy[frame_idx]) if frame_idx < len(audio_features.energy) else 0.0,
                'spectral_centroid': float(audio_features.spectral_centroid[frame_idx]) if frame_idx < len(audio_features.spectral_centroid) else 0.0,
                'pitch': float(audio_features.pitch[frame_idx]) if frame_idx < len(audio_features.pitch) else 0.0,
                'emotion_scores': audio_features.emotion_scores,
                'is_beat': self._is_beat_at_time(time_point, audio_features.beats)
            }

            timeline.append(timeline_point)

        return timeline

    def _is_beat_at_time(self, time_point: float, beats: List[float], tolerance: float = 0.1) -> bool:
        """åˆ¤æ–­æ—¶é—´ç‚¹æ˜¯å¦æ¥è¿‘èŠ‚æ‹"""
        for beat_time in beats:
            if abs(time_point - beat_time) <= tolerance:
                return True
        return False

    def _build_expression_file(
        self,
        audio_features: AudioFeatures,
        expressions: List[Dict],
        model_name: str
    ) -> Dict:
        """
        æ„å»ºæœ€ç»ˆçš„è¡¨æƒ…æ–‡ä»¶æ ¼å¼

        Args:
            audio_features: éŸ³é¢‘ç‰¹å¾
            expressions: è¡¨æƒ…å‚æ•°åˆ—è¡¨
            model_name: æ¨¡å‹åç§°

        Returns:
            Dict: å®Œæ•´çš„è¡¨æƒ…æ–‡ä»¶
        """
        expression_file = {
            "metadata": {
                "version": "1.0",
                "format": "geyan-suidong-expression",
                "model_name": model_name,
                "duration": audio_features.duration,
                "fps": 30,  # å‡è®¾30fps
                "total_frames": int(audio_features.duration * 30),
                "expression_count": len(expressions),
                "generated_at": self._get_current_timestamp(),
                "audio_analysis": {
                    "tempo": audio_features.tempo,
                    "emotion_scores": audio_features.emotion_scores,
                    "beat_count": len(audio_features.beats)
                }
            },
            "expressions": []
        }

        # å¤„ç†è¡¨æƒ…æ•°æ®
        for i, expr in enumerate(expressions):
            expression_entry = {
                "id": i,
                "timestamp": expr.get('timestamp', 0.0),
                "parameters": expr.get('parameters', {}),
                "transition_duration": expr.get('metadata', {}).get('transition_duration', 0.5),
                "easing": "easeInOutQuad",  # ç¼“åŠ¨å‡½æ•°
                "metadata": {
                    "reasoning": expr.get('metadata', {}).get('reasoning', ''),
                    "energy_level": self._get_energy_level_at_time(
                        expr.get('timestamp', 0.0),
                        audio_features
                    )
                }
            }

            expression_file["expressions"].append(expression_entry)

        # æ·»åŠ å…³é”®äº‹ä»¶æ ‡è®°
        expression_file["events"] = self._extract_key_events(audio_features)

        return expression_file

    def _get_energy_level_at_time(self, timestamp: float, audio_features: AudioFeatures) -> float:
        """è·å–æŒ‡å®šæ—¶é—´çš„èƒ½é‡çº§åˆ«"""
        if not audio_features.timestamps:
            return 0.0

        # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ—¶é—´ç´¢å¼•
        time_array = np.array(audio_features.timestamps)
        closest_idx = np.argmin(np.abs(time_array - timestamp))

        if closest_idx < len(audio_features.energy):
            return float(audio_features.energy[closest_idx])
        return 0.0

    def _extract_key_events(self, audio_features: AudioFeatures) -> List[Dict]:
        """æå–å…³é”®éŸ³ä¹äº‹ä»¶"""
        events = []

        # æ·»åŠ èŠ‚æ‹äº‹ä»¶
        for i, beat_time in enumerate(audio_features.beats):
            events.append({
                "type": "beat",
                "timestamp": float(beat_time),
                "index": i,
                "metadata": {
                    "tempo": audio_features.tempo
                }
            })

        # æ·»åŠ èƒ½é‡å³°å€¼äº‹ä»¶
        energy_threshold = np.percentile(audio_features.energy, 90)
        for i, energy in enumerate(audio_features.energy):
            if energy > energy_threshold:
                timestamp = audio_features.timestamps[i] if i < len(audio_features.timestamps) else 0
                events.append({
                    "type": "energy_peak",
                    "timestamp": float(timestamp),
                    "intensity": float(energy),
                    "metadata": {
                        "threshold": float(energy_threshold)
                    }
                })

        # æŒ‰æ—¶é—´æ’åº
        events.sort(key=lambda x: x['timestamp'])
        return events

    def _save_expression_file(self, expression_data: Dict, output_path: str) -> None:
        """ä¿å­˜è¡¨æƒ…æ–‡ä»¶"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(expression_data, f, indent=2, ensure_ascii=False)

        logger.info(f"è¡¨æƒ…æ–‡ä»¶å·²ä¿å­˜: {output_path}")

    def _get_current_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()

    def validate_expression_file(self, file_path: str) -> Dict:
        """
        éªŒè¯è¡¨æƒ…æ–‡ä»¶æ ¼å¼

        Args:
            file_path: è¡¨æƒ…æ–‡ä»¶è·¯å¾„

        Returns:
            Dict: éªŒè¯ç»“æœ
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ['metadata', 'expressions']
            missing_fields = [field for field in required_fields if field not in data]

            if missing_fields:
                return {
                    'valid': False,
                    'error': f'ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}'
                }

            # æ£€æŸ¥è¡¨æƒ…å‚æ•°
            expressions = data.get('expressions', [])
            invalid_expressions = []

            for i, expr in enumerate(expressions):
                if 'timestamp' not in expr or 'parameters' not in expr:
                    invalid_expressions.append(i)

            if invalid_expressions:
                return {
                    'valid': False,
                    'error': f'è¡¨æƒ… {invalid_expressions} æ ¼å¼æ— æ•ˆ'
                }

            return {
                'valid': True,
                'metadata': data.get('metadata', {}),
                'expression_count': len(expressions),
                'duration': data.get('metadata', {}).get('duration', 0)
            }

        except Exception as e:
            return {
                'valid': False,
                'error': f'æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}'
            }
```

### 4. FastAPI ä¸»åº”ç”¨

#### `backend/api/main.py`

```python
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from typing import Optional, Dict, Any
import uvicorn
import os
import shutil
import uuid
from pathlib import Path
import logging

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from ..core.audio_analyzer import AudioAnalyzer
from ..core.langchain_agent import ExpressionAgent
from ..core.expression_generator import ExpressionGenerator
from ..utils.file_utils import get_file_extension, validate_audio_file
from ..utils.config import get_settings
from ..models.response import ResponseModel, AudioAnalysisResponse, ExpressionGenerationResponse

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è·å–è®¾ç½®
settings = get_settings()

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="æ­Œé¢œéšåŠ¨ API",
    description="éŸ³ä¹è¡¨æƒ…ç”ŸæˆAPIæœåŠ¡",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥è®¾ç½®å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
audio_analyzer = AudioAnalyzer()
expression_agent = ExpressionAgent()
expression_generator = ExpressionGenerator(audio_analyzer, expression_agent)

# å­˜å‚¨å¤„ç†çŠ¶æ€
processing_status: Dict[str, Dict] = {}

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    logger.info("æ­Œé¢œéšåŠ¨ API æœåŠ¡å¯åŠ¨")

    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    Path(settings.upload_dir).mkdir(parents=True, exist_ok=True)
    Path(settings.expressions_dir).mkdir(parents=True, exist_ok=True)
    Path(settings.temp_dir).mkdir(parents=True, exist_ok=True)

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "æ­Œé¢œéšåŠ¨ API æœåŠ¡",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "timestamp": "2024-12-05"}

@app.post("/api/upload-audio", response_model=ResponseModel)
async def upload_audio(file: UploadFile = File(...)):
    """
    ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶

    Args:
        file: ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶

    Returns:
        ResponseModel: åŒ…å«æ–‡ä»¶IDçš„å“åº”
    """
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not validate_audio_file(file.filename):
            raise HTTPException(
                status_code=400,
                detail="ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼ï¼Œè¯·ä¸Šä¼  MP3, WAV, M4A æ ¼å¼çš„æ–‡ä»¶"
            )

        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶ID
        file_id = str(uuid.uuid4())
        file_extension = get_file_extension(file.filename)
        file_path = Path(settings.upload_dir) / f"{file_id}{file_extension}"

        # ä¿å­˜æ–‡ä»¶
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        # è®°å½•æ–‡ä»¶ä¿¡æ¯
        file_info = {
            "file_id": file_id,
            "original_name": file.filename,
            "file_path": str(file_path),
            "file_size": file_path.stat().st_size,
            "status": "uploaded"
        }

        processing_status[file_id] = file_info

        logger.info(f"éŸ³é¢‘æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file.filename} -> {file_id}")

        return ResponseModel(
            success=True,
            message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
            data={
                "file_id": file_id,
                "original_name": file.filename,
                "file_size": file_info["file_size"]
            }
        )

    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

@app.post("/api/analyze-audio/{file_id}", response_model=AudioAnalysisResponse)
async def analyze_audio(file_id: str):
    """
    åˆ†æéŸ³é¢‘æ–‡ä»¶

    Args:
        file_id: æ–‡ä»¶ID

    Returns:
        AudioAnalysisResponse: éŸ³é¢‘åˆ†æç»“æœ
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if file_id not in processing_status:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")

        file_info = processing_status[file_id]
        file_path = file_info["file_path"]

        if not Path(file_path).exists():
            raise HTTPException(status_code=404, detail="éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨")

        # æ›´æ–°çŠ¶æ€
        file_info["status"] = "analyzing"

        # åˆ†æéŸ³é¢‘
        logger.info(f"å¼€å§‹åˆ†æéŸ³é¢‘: {file_id}")
        audio_features = audio_analyzer.analyze(file_path)

        # æ„å»ºå“åº”æ•°æ®
        analysis_result = {
            "file_id": file_id,
            "duration": audio_features.duration,
            "tempo": audio_features.tempo,
            "beat_count": len(audio_features.beats),
            "emotion_scores": audio_features.emotion_scores,
            "energy_stats": {
                "mean": float(audio_features.energy.mean()),
                "max": float(audio_features.energy.max()),
                "min": float(audio_features.energy.min())
            },
            "spectral_stats": {
                "mean": float(audio_features.spectral_centroid.mean()),
                "max": float(audio_features.spectral_centroid.max()),
                "min": float(audio_features.spectral_centroid.min())
            }
        }

        # æ›´æ–°çŠ¶æ€
        file_info["status"] = "analyzed"
        file_info["analysis_result"] = analysis_result

        logger.info(f"éŸ³é¢‘åˆ†æå®Œæˆ: {file_id}, æ—¶é•¿: {audio_features.duration:.2f}ç§’")

        return AudioAnalysisResponse(
            success=True,
            message="éŸ³é¢‘åˆ†æå®Œæˆ",
            data=analysis_result
        )

    except Exception as e:
        logger.error(f"éŸ³é¢‘åˆ†æå¤±è´¥: {str(e)}")
        if file_id in processing_status:
            processing_status[file_id]["status"] = "error"
            processing_status[file_id]["error"] = str(e)
        raise HTTPException(status_code=500, detail=f"éŸ³é¢‘åˆ†æå¤±è´¥: {str(e)}")

@app.post("/api/generate-expression/{file_id}", response_model=ExpressionGenerationResponse)
async def generate_expression(
    file_id: str,
    background_tasks: BackgroundTasks,
    model_name: str = "default",
    time_resolution: float = 0.5,
    enable_smoothing: bool = True
):
    """
    ç”Ÿæˆè¡¨æƒ…æ–‡ä»¶

    Args:
        file_id: æ–‡ä»¶ID
        model_name: Live2Dæ¨¡å‹åç§°
        time_resolution: æ—¶é—´åˆ†è¾¨ç‡
        enable_smoothing: æ˜¯å¦å¯ç”¨å¹³æ»‘å¤„ç†

    Returns:
        ExpressionGenerationResponse: è¡¨æƒ…ç”Ÿæˆç»“æœ
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶çŠ¶æ€
        if file_id not in processing_status:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")

        file_info = processing_status[file_id]

        if file_info.get("status") != "analyzed":
            raise HTTPException(status_code=400, detail="è¯·å…ˆå®ŒæˆéŸ³é¢‘åˆ†æ")

        file_path = file_info["file_path"]

        # ç”Ÿæˆè¾“å‡ºè·¯å¾„
        expression_id = str(uuid.uuid4())
        output_path = Path(settings.expressions_dir) / f"{expression_id}.json"

        # æ›´æ–°çŠ¶æ€
        file_info["status"] = "generating"

        # ç”Ÿæˆè¡¨æƒ…æ–‡ä»¶
        logger.info(f"å¼€å§‹ç”Ÿæˆè¡¨æƒ…æ–‡ä»¶: {file_id} -> {expression_id}")

        result = expression_generator.generate_expression_file(
            audio_path=file_path,
            output_path=str(output_path),
            model_name=model_name,
            time_resolution=time_resolution,
            enable_smoothing=enable_smoothing
        )

        if result["success"]:
            # æ›´æ–°çŠ¶æ€
            file_info["status"] = "completed"
            file_info["expression_id"] = expression_id
            file_info["expression_path"] = str(output_path)

            response_data = {
                "file_id": file_id,
                "expression_id": expression_id,
                "expression_file": str(output_path),
                "duration": result["duration"],
                "expression_count": result["expression_count"],
                "model_name": result["model_name"],
                "metadata": result["metadata"]
            }

            logger.info(f"è¡¨æƒ…æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {expression_id}")

            return ExpressionGenerationResponse(
                success=True,
                message="è¡¨æƒ…æ–‡ä»¶ç”ŸæˆæˆåŠŸ",
                data=response_data
            )
        else:
            # ç”Ÿæˆå¤±è´¥
            file_info["status"] = "error"
            file_info["error"] = result["error"]

            raise HTTPException(
                status_code=500,
                detail=f"è¡¨æƒ…ç”Ÿæˆå¤±è´¥: {result['error']}"
            )

    except Exception as e:
        logger.error(f"è¡¨æƒ…ç”Ÿæˆå¤±è´¥: {str(e)}")
        if file_id in processing_status:
            processing_status[file_id]["status"] = "error"
            processing_status[file_id]["error"] = str(e)
        raise HTTPException(status_code=500, detail=f"è¡¨æƒ…ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.get("/api/expression/{expression_id}")
async def get_expression_file(expression_id: str):
    """
    è·å–è¡¨æƒ…æ–‡ä»¶

    Args:
        expression_id: è¡¨æƒ…æ–‡ä»¶ID

    Returns:
        FileResponse: è¡¨æƒ…æ–‡ä»¶
    """
    expression_path = Path(settings.expressions_dir) / f"{expression_id}.json"

    if not expression_path.exists():
        raise HTTPException(status_code=404, detail="è¡¨æƒ…æ–‡ä»¶ä¸å­˜åœ¨")

    return FileResponse(
        path=expression_path,
        media_type="application/json",
        filename=f"{expression_id}.json"
    )

@app.get("/api/status/{file_id}")
async def get_processing_status(file_id: str):
    """
    è·å–å¤„ç†çŠ¶æ€

    Args:
        file_id: æ–‡ä»¶ID

    Returns:
        Dict: å¤„ç†çŠ¶æ€ä¿¡æ¯
    """
    if file_id not in processing_status:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")

    return processing_status[file_id]

@app.get("/api/models")
async def list_available_models():
    """
    è·å–å¯ç”¨çš„Live2Dæ¨¡å‹åˆ—è¡¨

    Returns:
        Dict: æ¨¡å‹åˆ—è¡¨
    """
    # è¿™é‡Œåº”è¯¥æ‰«æmodelsç›®å½•è·å–å®é™…çš„æ¨¡å‹åˆ—è¡¨
    models = [
        {
            "id": "default",
            "name": "é»˜è®¤è§’è‰²",
            "description": "æ ‡å‡†çš„è™šæ‹Ÿè§’è‰²æ¨¡å‹",
            "preview_image": "/models/default/preview.png"
        },
        {
            "id": "hiyori",
            "name": "Hiyori",
            "description": "å¯çˆ±çš„æ—¥ç³»è™šæ‹Ÿè§’è‰²",
            "preview_image": "/models/hiyori/preview.png"
        }
    ]

    return {
        "success": True,
        "data": {
            "models": models,
            "count": len(models)
        }
    }

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=settings.api_debug
    )
```

## ğŸ§ª æµ‹è¯•ç¤ºä¾‹

### `backend/tests/test_audio_analyzer.py`

```python
import pytest
import numpy as np
from pathlib import Path
from backend.core.audio_analyzer import AudioAnalyzer

class TestAudioAnalyzer:

    def test_analyzer_initialization(self):
        """æµ‹è¯•åˆ†æå™¨åˆå§‹åŒ–"""
        analyzer = AudioAnalyzer()
        assert analyzer.sample_rate == 44100
        assert analyzer.hop_length == 512

    @pytest.mark.skipif(not Path("tests/data/test.wav").exists(),
                       reason="æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
    def test_audio_analysis(self):
        """æµ‹è¯•éŸ³é¢‘åˆ†æåŠŸèƒ½"""
        analyzer = AudioAnalyzer()
        features = analyzer.analyze("tests/data/test.wav")

        assert features.duration > 0
        assert features.tempo > 0
        assert len(features.beats) > 0
        assert len(features.energy) > 0
        assert isinstance(features.emotion_scores, dict)
```

---

è¿™ä¸ªåç«¯å¼€å‘æŒ‡å—ä¸ºæ‚¨æä¾›äº†å®Œæ•´çš„åç«¯å¼€å‘æ¡†æ¶å’Œå®ç°æ–¹æ¡ˆã€‚æ¯ä¸ªæ¨¡å—éƒ½æœ‰è¯¦ç»†çš„ä»£ç ç¤ºä¾‹å’Œè¯´æ˜ï¼Œå¯ä»¥ç›´æ¥ç”¨äºé¡¹ç›®å¼€å‘ã€‚
